"""
Training strategies for different conditioning modes (with prev-shot support).

- StandardTrainingStrategy:
    prev-shot(í´ë¦°) + curr-shot(ë…¸ì´ì¦ˆ) ë¥¼ ì‹œí€€ìŠ¤ ì°¨ì›ìœ¼ë¡œ concat
    â†’ prev êµ¬ê°„ì€ ì „ë¶€ conditioning(True), currì˜ ì²« í”„ë ˆì„ë§Œ ì„ íƒì  conditioning
    â†’ íƒ€ê¹ƒ/ë¡œìŠ¤ëŠ” curr-shotì—ë§Œ ì ìš© (ë§ˆìŠ¤í‚¹ë¨)

- ReferenceVideoTrainingStrategy (IC-LoRA ìŠ¤íƒ€ì¼):
    ê¸°ì¡´ ë³„ë„ ref_latents ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    ì—†ìœ¼ë©´ datasetì´ ì œê³µí•œ prev-shotì„ ë ˆí¼ëŸ°ìŠ¤ë¡œ ì‚¬ìš©(ìë™ í´ë°±)
"""

import random
from abc import ABC, abstractmethod
from typing import Any

import torch
from pydantic import BaseModel, computed_field
from torch import Tensor

from ltxv_trainer import logger
from ltxv_trainer.config import ConditioningConfig
from ltxv_trainer.ltxv_utils import get_rope_scale_factors, prepare_video_coordinates
from ltxv_trainer.timestep_samplers import TimestepSampler

DEFAULT_FPS = 24  # FPS ë©”íƒ€ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’


# --------------------------
# Batch container (ë™ì¼)
# --------------------------
class TrainingBatch(BaseModel):
    latents: Tensor                # (B, Seq, D)  # ex) Seq = (prev_seq + curr_seq)
    targets: Tensor                # (B, Seq, D)  # prev êµ¬ê°„ì€ 0, curr êµ¬ê°„ë§Œ ìœ íš¨

    prompt_embeds: Tensor
    prompt_attention_mask: Tensor

    timesteps: Tensor              # (B, Seq)     # prev=0, curr=sampled
    sigmas: Tensor                 # (B, 1, 1)    # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„(ë¸Œë¡œë“œìºìŠ¤íŠ¸ ìš©ë„)

    conditioning_mask: Tensor      # (B, Seq)     # True=conditioning(prev ì „ì²´ + curr ì¼ë¶€)

    num_frames: int
    height: int
    width: int
    fps: float

    rope_interpolation_scale: list[float]
    video_coords: Tensor | None = None

    @computed_field
    @property
    def batch_size(self) -> int:
        return self.latents.shape[0]

    @computed_field
    @property
    def sequence_length(self) -> int:
        return self.latents.shape[1]

    model_config = {"arbitrary_types_allowed": True}


# --------------------------
# Base strategy
# --------------------------
class TrainingStrategy(ABC):
    def __init__(self, conditioning_config: ConditioningConfig):
        self.conditioning_config = conditioning_config

    @abstractmethod
    def get_data_sources(self) -> list[str] | dict[str, str]:
        """
        ex) ["latents","conditions"]  or  {"latents":"latents","conditions":"conditions","ref_latents_dir":"ref_latents"}
        # dataset.sample_files : {"latent_conditions":[...], "text_conditions":[...]} ì™€ì˜ ë§¤í•‘ì€ Dataset ìª½ì—ì„œ ì²˜ë¦¬
        """

    @abstractmethod
    def prepare_batch(self, batch: dict[str, Any], timestep_sampler: TimestepSampler) -> TrainingBatch:
        ...

    def _create_timesteps_from_conditioning_mask(
        self, conditioning_mask: Tensor, sampled_timestep_values: Tensor
    ) -> Tensor:
        """
        # conditioning_mask: (B, Seq)  True=conditioning â†’ timestep=0
        # sampled_timestep_values: (B,) â†’ ê° ë°°ì¹˜ì˜ curr êµ¬ê°„ì— ë³µì œ

        return: timesteps (B, Seq)
        """
        expanded = sampled_timestep_values.unsqueeze(1).expand_as(conditioning_mask)  # (B, Seq)
        return torch.where(conditioning_mask, 0, expanded)

    def _create_first_frame_conditioning_mask(
        self, batch_size: int, sequence_length: int, height: int, width: int, device: torch.device
    ) -> Tensor:
        """
        curr-shot ë‚´ë¶€ì—ì„œ 'ì²« í”„ë ˆì„'ì„ conditioning ì²˜ë¦¬í• ì§€ í™•ë¥ ì ìœ¼ë¡œ ê²°ì •
        # True ë²”ìœ„: ì²« í”„ë ˆì„ì˜ (H*W) í† í° â†’ ex) (B, H*W)=True, ë‚˜ë¨¸ì§€ False
        """
        mask = torch.zeros(batch_size, sequence_length, dtype=torch.bool, device=device)
        if (
            self.conditioning_config.first_frame_conditioning_p > 0
            and random.random() < self.conditioning_config.first_frame_conditioning_p
        ):
            first_frame_end = min(height * width, sequence_length)  # ì•ˆì „ì¥ì¹˜
            mask[:, :first_frame_end] = True
        return mask

    @staticmethod
    def prepare_model_inputs(batch: TrainingBatch) -> dict[str, Any]:
        """
        ëª¨ë¸ í¬ì›Œë“œ ì…ë ¥ ê·œê²© ìœ ì§€
        """
        return {
            "hidden_states": batch.latents,
            "encoder_hidden_states": batch.prompt_embeds,
            "timestep": batch.timesteps,
            "encoder_attention_mask": batch.prompt_attention_mask,
            "num_frames": batch.num_frames,
            "height": batch.height,
            "width": batch.width,
            "rope_interpolation_scale": batch.rope_interpolation_scale,
            "video_coords": batch.video_coords,
            "return_dict": False,
        }

    @abstractmethod
    def compute_loss(self, model_pred: Tensor, batch: TrainingBatch) -> Tensor:
        ...


# --------------------------
# ìœ í‹¸: dataset â†’ í†µì¼ ì–¸íŒ©
# --------------------------
def _unpack_latent_entry(entry: Any) -> tuple[Tensor, int, int, int, float]:
    """
    entry ì˜ˆì‹œ
    1) í…ì„œ ì§ì ‘ ì €ì¥ëœ ê²½ìš°: entry = <Tensor: (B, Seq, D)>
    2) dictë¡œ ë©”íƒ€ í¬í•¨: entry = {
            "latents": Tensor(B, Seq, D),
            "num_frames": Tensor([F,...]),
            "height": Tensor([H,...]),
            "width": Tensor([W,...]),
            "fps": Tensor([..., ...])  # ë˜ëŠ” ì—†ìŒ
       }

    return: (latents, F, H, W, fps)
    """
    if torch.is_tensor(entry):
        # ë©”íƒ€ê°€ ì—†ìœ¼ë©´ í•©ë¦¬ì  ê¸°ë³¸ê°’ ì¶”ì • í•„ìš” â†’ ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ë”ë¯¸ ê°’
        # ex) (B, F*H*W, D) êµ¬ì¡°ë¼ ê°€ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, í˜¸ì¶œë¶€ì—ì„œ ë©”íƒ€ë¥¼ ë³„ë„ë¡œ ì „ë‹¬í•˜ëŠ” í¸ì´ ë” ì •í™•.
        raise ValueError("Latent entry missing meta. Save dict with num_frames/height/width/fps.")
    else:
        lat = entry["latents"]                   # (B, Seq, D)
        F = int(entry["num_frames"][0].item())
        H = int(entry["height"][0].item())
        W = int(entry["width"][0].item())
        fps_t = entry.get("fps", None)
        if fps_t is not None and not torch.all(fps_t == fps_t[0]):
            logger.warning(f"Different FPS in batch: {fps_t.tolist()}, using first.")
        fps = float((fps_t[0].item() if fps_t is not None else DEFAULT_FPS))
        return lat, F, H, W, fps


def _unpack_condition_entry(conds: Any) -> tuple[Tensor, Tensor]:
    """
    conds ì˜ˆì‹œ
    1) scene-levelë§Œ ìˆëŠ” ê²½ìš°: conds = {"prompt_embeds": (B, T, D), "prompt_attention_mask": (B, T)}
    2) prev/curr êµ¬ë¶„ ì €ì¥: conds = {
            "current_shot": {"prompt_embeds":..., "prompt_attention_mask":...},
            "prev_shot":    {"prompt_embeds":..., "prompt_attention_mask":...} (ì˜µì…˜)
       }
    """
    if "current_shot" in conds:
        c = conds["current_shot"]
        return c["prompt_embeds"], c["prompt_attention_mask"]
    else:
        return conds["prompt_embeds"], conds["prompt_attention_mask"]


def _concat_prev_curr(prev_lat: Tensor | None, curr_lat: Tensor) -> tuple[Tensor, int, int]:
    """
    prev_lat: (B, Pseq, D) ë˜ëŠ” None
    curr_lat: (B, Cseq, D)
    return: (concat, Pseq, Cseq)

    # concat = [prev(clean) | curr(noisy)] @ seq-dim
    """
    if prev_lat is None:
        return curr_lat, 0, curr_lat.shape[1]
    return torch.cat([prev_lat, curr_lat], dim=1), prev_lat.shape[1], curr_lat.shape[1]


# ----------------------------------------
# Standard: prev-shot ì§€ì› (ìƒˆ ê·œì•½ì— ë§ê²Œ)
# ----------------------------------------
class StandardTrainingStrategy(TrainingStrategy):
    def __init__(self, conditioning_config: ConditioningConfig):
        super().__init__(conditioning_config)

    def get_data_sources(self) -> list[str]:
        """
        í‘œì¤€ í•™ìŠµì— í•„ìš”í•œ ì†ŒìŠ¤:
        - "latents": {"current_shot": {...}, "prev_shot": {... or SOS}}  # â† Datasetì´ ë³´ì¥
        - "conditions": scene/text ì¡°ê±´ (prev êµ¬ë¶„ì´ ìˆë”ë¼ë„ currë§Œ ì“°ë©´ ì¶©ë¶„)
        """
        return ["latents", "conditions"]

    def prepare_batch(self, batch: dict[str, Any], timestep_sampler: TimestepSampler) -> TrainingBatch:
        # 1) Latents ì–¸íŒ©
        lat_node = batch["latents"]  # ex) {"current_shot": <dict>, "prev_shot": <dict or Tensor or None>, ...}
        curr_lat, F, H, W, fps = _unpack_latent_entry(lat_node["current_shot"])
        # ex) curr_lat: (B, Cseq, D)

        prev_lat = None
        if lat_node.get("prev_shot", None) is not None:
            # prev_shotì´ SOS í…ì„œ/ì‹¤ìƒ· ëª¨ë‘ ê°€ëŠ¥ (Datasetì´ ìƒì„±)
            prev_lat, _, _, _, _ = _unpack_latent_entry(lat_node["prev_shot"])
            # ex) prev_lat: (B, Pseq, D)

        # 2) Conditions ì–¸íŒ© (scene-levelì´ë©´ currì™€ ë™ì¼)
        prompt_embeds, prompt_attention_mask = _unpack_condition_entry(batch["conditions"])

        # 3) ë…¸ì´ì¦ˆ ìƒ˜í”Œ & ì‹œê·¸ë§ˆ (currì—ë§Œ ì ìš©)
        sigmas = timestep_sampler.sample_for(curr_lat)         # (B, Cseq, 1) ë˜ëŠ” ì „ëµ êµ¬í˜„ì— ë”°ë¼
        # â†“ ì•„ë˜ ì—°ì‚°ì—ì„œ (B,1,1) ë¸Œë¡œë“œìºìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ reshape
        if sigmas.dim() > 3:
            raise ValueError("Unexpected sigma shape; expected (B,1,1) style broadcast.")
        sigmas = sigmas.view(curr_lat.shape[0], 1, 1)          # (B,1,1)

        noise = torch.randn_like(curr_lat, device=curr_lat.device)  # (B, Cseq, D)
        noisy_curr = (1 - sigmas) * curr_lat + sigmas * noise       # (B, Cseq, D)

        # 4) curr ë‚´ë¶€ 'ì²« í”„ë ˆì„ conditioning' ì ìš©: ì²« í”„ë ˆì„ í† í°ì€ í´ë¦°ìœ¼ë¡œ ëŒ€ì²´
        first_mask_curr = self._create_first_frame_conditioning_mask(
            batch_size=curr_lat.shape[0],
            sequence_length=curr_lat.shape[1],
            height=H,
            width=W,
            device=curr_lat.device,
        )  # (B, Cseq) True=clean keep
        noisy_curr = torch.where(first_mask_curr.unsqueeze(-1), curr_lat, noisy_curr)

        # 5) prev + curr concat
        concat_lat, Pseq, Cseq = _concat_prev_curr(prev_lat, noisy_curr)  # (B, P+C, D)

        # 6) conditioning mask êµ¬ì„±
        #    prev ì „ì²´ True(ì¡°ê±´), currì€ first_frameë§Œ True
        if Pseq > 0:
            prev_mask = torch.ones(curr_lat.shape[0], Pseq, dtype=torch.bool, device=curr_lat.device)  # (B, Pseq)=True
            conditioning_mask = torch.cat([prev_mask, first_mask_curr], dim=1)  # (B, P+C)
        else:
            conditioning_mask = first_mask_curr  # (B, C)

        # 7) íƒ€ê¹ƒ êµ¬ì„±: prev êµ¬ê°„ì€ 0 (ë§ˆìŠ¤í‚¹ë˜ë¯€ë¡œ ì˜í–¥ X), curr êµ¬ê°„ì€ (noise - clean)
        targets_curr = noise - curr_lat  # (B, Cseq, D)
        if Pseq > 0:
            zeros_prev = torch.zeros(curr_lat.shape[0], Pseq, curr_lat.shape[2], device=curr_lat.device, dtype=targets_curr.dtype)
            targets = torch.cat([zeros_prev, targets_curr], dim=1)  # (B, P+C, D)
        else:
            targets = targets_curr  # (B, C, D)

        # 8) timestep ìƒì„±: prev=0, curr=round(sigmas*1000)
        sampled_t = torch.round(sigmas.squeeze(-1).squeeze(-1) * 1000.0).long()  # (B,)
        timesteps = self._create_timesteps_from_conditioning_mask(conditioning_mask, sampled_t)  # (B, P+C)

        # 9) ROPE scale & video coords (prev+curr ê¸¸ì´ì— ë§ì¶”ì–´ ì¤€ë¹„)
        rope_scale = get_rope_scale_factors(fps)
        # seq_mult = 2 if Pseq > 0 else 1  # prevë¥¼ ë¶™ì´ë©´ 2ë°°

        return TrainingBatch(
            latents=concat_lat,
            targets=targets,
            prompt_embeds=prompt_embeds,
            prompt_attention_mask=prompt_attention_mask,
            timesteps=timesteps,
            sigmas=sigmas,
            conditioning_mask=conditioning_mask,
            num_frames=F,
            height=H,
            width=W,
            fps=fps,
            rope_interpolation_scale=rope_scale,
            video_coords=None,
        )

    def compute_loss(self, model_pred: Tensor, batch: TrainingBatch) -> Tensor:
        """
        ë§ˆìŠ¤í‚¹ MSE
        - prev ì „ì²´ & currì˜ ì²« í”„ë ˆì„(ì¡°ê±´)ì€ ì œì™¸
        - targetsëŠ” prev êµ¬ê°„ 0ìœ¼ë¡œ ì±„ì›Œì ¸ ìˆìŒ (ì•ˆì „)
        """
        loss = (model_pred - batch.targets).pow(2)                    # (B, Seq, D)
        loss_mask = (~batch.conditioning_mask.unsqueeze(-1)).float()  # (B, Seq, 1)
        loss = loss.mul(loss_mask).div(loss_mask.mean())              # í‰ê·  ì •ê·œí™”
        return loss.mean()


# ------------------------------------------------------
# ReferenceVideo (IC-LoRA): prev-shot ìë™ í´ë°± ì§€ì›
# ------------------------------------------------------
class ReferenceVideoTrainingStrategy(TrainingStrategy):
    def __init__(self, conditioning_config: ConditioningConfig):
        super().__init__(conditioning_config)

    def get_data_sources(self) -> dict[str, str]:
        """
        ê¸°ì¡´: ë³„ë„ ref_latents_dir í•„ìš”.
        ë³´ê°•: ref_latents_dirê°€ ì—†ê±°ë‚˜ batchì— ì—†ìœ¼ë©´ datasetì˜ prev_shotì„ ë ˆí¼ëŸ°ìŠ¤ë¡œ ì‚¬ìš©(í´ë°±)
        """
        mapping = {"latents": "latents", "conditions": "conditions"}
        if getattr(self.conditioning_config, "reference_latents_dir", None):
            mapping[self.conditioning_config.reference_latents_dir] = "ref_latents"
        return mapping

    def prepare_batch(self, batch: dict[str, dict[str, Tensor]], timestep_sampler: TimestepSampler) -> TrainingBatch:
        lat_node = batch["latents"]

        # 1) íƒ€ê¹ƒ(latents) ì–¸íŒ© (curr)
        curr_lat, F, H, W, fps = _unpack_latent_entry(lat_node["current_shot"])  # (B, Cseq, D)

        # 2) ë ˆí¼ëŸ°ìŠ¤ í™•ë³´
        ref_lat = None
        # (a) ì„¤ì •ëœ ë³„ë„ ë””ë ‰í† ë¦¬ì—ì„œ ì œê³µë˜ë©´ ì‚¬ìš©
        if "ref_latents" in batch:
            ref_lat, _, _, _, _ = _unpack_latent_entry(batch["ref_latents"])     # (B, Rseq, D)
        # (b) ì—†ìœ¼ë©´ prev_shotì„ ë ˆí¼ëŸ°ìŠ¤ë¡œ í´ë°±
        elif lat_node.get("prev_shot", None) is not None:
            ref_lat, _, _, _, _ = _unpack_latent_entry(lat_node["prev_shot"])    # (B, Pseq, D)

        # 3) í…ìŠ¤íŠ¸ ì¡°ê±´
        prompt_embeds, prompt_attention_mask = _unpack_condition_entry(batch["conditions"])

        # 4) currì—ë§Œ ë…¸ì´ì¦ˆ
        sigmas = timestep_sampler.sample_for(curr_lat)  # ê¸°ëŒ€ ëª¨ì–‘ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        sigmas = sigmas.view(curr_lat.shape[0], 1, 1)
        noise = torch.randn_like(curr_lat, device=curr_lat.device)
        noisy_curr = (1 - sigmas) * curr_lat + sigmas * noise

        # 5) curr ì²« í”„ë ˆì„ conditioning
        first_mask_curr = self._create_first_frame_conditioning_mask(
            batch_size=curr_lat.shape[0],
            sequence_length=curr_lat.shape[1],
            height=H,
            width=W,
            device=curr_lat.device,
        )
        noisy_curr = torch.where(first_mask_curr.unsqueeze(-1), curr_lat, noisy_curr)

        # 6) ref(í•­ìƒ conditioning) + noisy_curr concat
        if ref_lat is None:
            # ë ˆí¼ëŸ°ìŠ¤ê°€ ì „í˜€ ì—†ë‹¤ë©´ í‘œì¤€ì „ëµê³¼ ë™ì¼ ë™ì‘ (ê²½ê³ )
            logger.warning("No explicit ref_latents nor prev_shot; falling back to Standard-like behavior.")
            ref_mask = None
            combined, Rseq, Cseq = _concat_prev_curr(None, noisy_curr)
        else:
            combined, Rseq, Cseq = _concat_prev_curr(ref_lat, noisy_curr)
            ref_mask = torch.ones(curr_lat.shape[0], Rseq, dtype=torch.bool, device=curr_lat.device)

        # 7) conditioning mask
        if Rseq > 0:
            conditioning_mask = torch.cat([ref_mask, first_mask_curr], dim=1)  # (B, R+C)
        else:
            conditioning_mask = first_mask_curr  # (B, C)

        # 8) targets (ref êµ¬ê°„ 0, curr êµ¬ê°„ noise - clean)
        targets_curr = noise - curr_lat
        if Rseq > 0:
            zeros_ref = torch.zeros(curr_lat.shape[0], Rseq, curr_lat.shape[2], device=curr_lat.device, dtype=targets_curr.dtype)
            targets = torch.cat([zeros_ref, targets_curr], dim=1)
        else:
            targets = targets_curr

        # 9) timesteps
        sampled_t = torch.round(sigmas.squeeze(-1).squeeze(-1) * 1000.0).long()  # (B,)
        timesteps = self._create_timesteps_from_conditioning_mask(conditioning_mask, sampled_t)  # (B, R+C)

        # 10) ROPE & coords (ref+curr 2ë°°)
        rope_scale = get_rope_scale_factors(fps)
        seq_mult = 2 if Rseq > 0 else 1
        raw_coords = prepare_video_coordinates(
            num_frames=F, height=H, width=W,
            batch_size=combined.shape[0],
            sequence_multiplier=seq_mult,
            device=combined.device,
        )
        prescaled_f = raw_coords[..., 0] * rope_scale[0]
        prescaled_h = raw_coords[..., 1] * rope_scale[1]
        prescaled_w = raw_coords[..., 2] * rope_scale[2]
        video_coords = torch.stack([prescaled_f, prescaled_h, prescaled_w], dim=1)  # (B, 3, R+C)

        return TrainingBatch(
            latents=combined,
            targets=targets,
            prompt_embeds=prompt_embeds,
            prompt_attention_mask=prompt_attention_mask,
            timesteps=timesteps,
            sigmas=sigmas,
            conditioning_mask=conditioning_mask,
            num_frames=F,
            height=H,
            width=W,
            fps=fps,
            rope_interpolation_scale=rope_scale,
            video_coords=video_coords,
        )

    def compute_loss(self, model_pred: Tensor, batch: TrainingBatch) -> Tensor:
        """
        ë§ˆìŠ¤í‚¹ MSE (ref + curr)
        - ref êµ¬ê°„: conditioning â†’ ë¡œìŠ¤ ì œì™¸
        - curr ì²« í”„ë ˆì„: conditioning â†’ ë¡œìŠ¤ ì œì™¸
        """
        loss = (model_pred - batch.targets).pow(2)
        loss_mask = (~batch.conditioning_mask.unsqueeze(-1)).float()
        loss = loss.mul(loss_mask).div(loss_mask.mean())
        return loss.mean()


# --------------- Factory ---------------
def get_training_strategy(conditioning_config: ConditioningConfig) -> TrainingStrategy:
    mode = conditioning_config.mode
    if mode == "none":
        strategy = StandardTrainingStrategy(conditioning_config)
    elif mode == "reference_video":
        strategy = ReferenceVideoTrainingStrategy(conditioning_config)
    else:
        raise ValueError(f"Unknown conditioning mode: {mode}")

    logger.debug(f"ğŸ¯ Using {strategy.__class__.__name__}")
    return strategy
