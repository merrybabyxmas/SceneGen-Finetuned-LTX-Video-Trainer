"""
Training strategies for different conditioning modes (with prev-shot support).

- StandardTrainingStrategy:
    prev-shot(í´ë¦°) + curr-shot(ë…¸ì´ì¦ˆ) ë¥¼ ì‹œí€€ìŠ¤ ì°¨ì›ìœ¼ë¡œ concat
    â†’ prev êµ¬ê°„ì€ ì „ë¶€ conditioning(True), currì˜ ì²« í”„ë ˆì„ë§Œ ì„ íƒì  conditioning
    â†’ íƒ€ê¹ƒ/ë¡œìŠ¤ëŠ” curr-shotì—ë§Œ ì ìš© (ë§ˆìŠ¤í‚¹ë¨)

- ReferenceVideoTrainingStrategy (IC-LoRA ìŠ¤íƒ€ì¼):
    ê¸°ì¡´ ë³„ë„ ref_latents ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    ì—†ìœ¼ë©´ datasetì´ ì œê³µí•œ prev-shotì„ ë ˆí¼ëŸ°ìŠ¤ë¡œ ì‚¬ìš©(ìë™ í´ë°±)
"""

import random
from abc import ABC, abstractmethod
from typing import Any

import torch
from pydantic import BaseModel, computed_field
from torch import Tensor

from ltxv_trainer import logger
from ltxv_trainer.config import ConditioningConfig
from ltxv_trainer.ltxv_utils import get_rope_scale_factors, prepare_video_coordinates
from ltxv_trainer.timestep_samplers import TimestepSampler, UniformTimestepSampler

DEFAULT_FPS = 24  # FPS ë©”íƒ€ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’


# --------------------------
# Batch container (ë™ì¼)
# --------------------------
class TrainingBatch(BaseModel):
    latents: Tensor                # (B, Seq, D)  # ex) Seq = (prev_seq + curr_seq)
    targets: Tensor                # (B, Seq, D)  # prev êµ¬ê°„ì€ 0, curr êµ¬ê°„ë§Œ ìœ íš¨

    prompt_embeds: Tensor
    prompt_attention_mask: Tensor

    timesteps: Tensor              # (B, Seq)     # prev=0, curr=sampled
    sigmas: Tensor                 # (B, 1, 1)    # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„(ë¸Œë¡œë“œìºìŠ¤íŠ¸ ìš©ë„)

    conditioning_mask: Tensor      # (B, Seq)     # True=conditioning(prev ì „ì²´ + curr ì¼ë¶€)

    num_frames: int
    height: int
    width: int
    fps: float

    rope_interpolation_scale: list[float]
    video_coords: Tensor | None = None

    @computed_field
    @property
    def batch_size(self) -> int:
        return self.latents.shape[0]

    @computed_field
    @property
    def sequence_length(self) -> int:
        return self.latents.shape[1]

    model_config = {"arbitrary_types_allowed": True}


# --------------------------
# Base strategy
# --------------------------
class TrainingStrategy(ABC):
    def __init__(self, conditioning_config: ConditioningConfig):
        self.conditioning_config = conditioning_config

    @abstractmethod
    def get_data_sources(self) -> list[str] | dict[str, str]:
        """
        ex) ["latents","conditions"]  or  {"latents":"latents","conditions":"conditions","ref_latents_dir":"ref_latents"}
        # dataset.sample_files : {"latent_conditions":[...], "text_conditions":[...]} ì™€ì˜ ë§¤í•‘ì€ Dataset ìª½ì—ì„œ ì²˜ë¦¬
        """

    @abstractmethod
    def prepare_batch(self, batch: dict[str, Any], timestep_sampler: TimestepSampler) -> TrainingBatch:
        ...

    def _create_timesteps_from_conditioning_mask(
        self, conditioning_mask: Tensor, sampled_timestep_values: Tensor
    ) -> Tensor:
        """
        # conditioning_mask: (B, Seq)  True=conditioning â†’ timestep=0
        # sampled_timestep_values: (B,) â†’ ê° ë°°ì¹˜ì˜ curr êµ¬ê°„ì— ë³µì œ

        return: timesteps (B, Seq)
        """
        expanded = sampled_timestep_values.unsqueeze(1).expand_as(conditioning_mask)  # (B, Seq)
        return torch.where(conditioning_mask, 0, expanded)

    def _create_first_frame_conditioning_mask(
        self, batch_size: int, sequence_length: int, height: int, width: int, device: torch.device
    ) -> Tensor:
        """
        curr-shot ë‚´ë¶€ì—ì„œ 'ì²« í”„ë ˆì„'ì„ conditioning ì²˜ë¦¬í• ì§€ í™•ë¥ ì ìœ¼ë¡œ ê²°ì •
        # True ë²”ìœ„: ì²« í”„ë ˆì„ì˜ (H*W) í† í° â†’ ex) (B, H*W)=True, ë‚˜ë¨¸ì§€ False
        """
        mask = torch.zeros(batch_size, sequence_length, dtype=torch.bool, device=device)
        if (
            self.conditioning_config.first_frame_conditioning_p > 0
            and random.random() < self.conditioning_config.first_frame_conditioning_p
        ):
            first_frame_end = min(height * width, sequence_length)  # ì•ˆì „ì¥ì¹˜
            mask[:, :first_frame_end] = True
        return mask

    @staticmethod
    def prepare_model_inputs(batch: TrainingBatch) -> dict[str, Any]:
        """
        ëª¨ë¸ í¬ì›Œë“œ ì…ë ¥ ê·œê²© ìœ ì§€
        """
        return {
            "hidden_states": batch.latents,
            "encoder_hidden_states": batch.prompt_embeds,
            "timestep": batch.timesteps,
            "encoder_attention_mask": batch.prompt_attention_mask,
            "num_frames": batch.num_frames,
            "height": batch.height,
            "width": batch.width,
            "rope_interpolation_scale": batch.rope_interpolation_scale,
            "video_coords": batch.video_coords,
            "return_dict": False,
        }

    @abstractmethod
    def compute_loss(self, model_pred: Tensor, batch: TrainingBatch) -> Tensor:
        ...


# --------------------------
# ìœ í‹¸: dataset â†’ í†µì¼ ì–¸íŒ©
# --------------------------
def _unpack_latent_entry(entry: Any) -> tuple[Tensor, int, int, int, float]:
    """
    entry ì˜ˆì‹œ
    1) í…ì„œ ì§ì ‘ ì €ì¥ëœ ê²½ìš°: entry = <Tensor: (B, Seq, D)>
    2) dictë¡œ ë©”íƒ€ í¬í•¨: entry = {
            "latents": Tensor(B, Seq, D),
            "num_frames": Tensor([F,...]),
            "height": Tensor([H,...]),
            "width": Tensor([W,...]),
            "fps": Tensor([..., ...])  # ë˜ëŠ” ì—†ìŒ
       }

    return: (latents, F, H, W, fps)
    """
    if torch.is_tensor(entry):
        # ë©”íƒ€ê°€ ì—†ìœ¼ë©´ í•©ë¦¬ì  ê¸°ë³¸ê°’ ì¶”ì • í•„ìš” â†’ ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ë”ë¯¸ ê°’
        # ex) (B, F*H*W, D) êµ¬ì¡°ë¼ ê°€ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, í˜¸ì¶œë¶€ì—ì„œ ë©”íƒ€ë¥¼ ë³„ë„ë¡œ ì „ë‹¬í•˜ëŠ” í¸ì´ ë” ì •í™•.
        raise ValueError("Latent entry missing meta. Save dict with num_frames/height/width/fps.")
    else:
        lat = entry["latents"]                   # (B, Seq, D)
        F = int(entry["num_frames"][0].item())
        H = int(entry["height"][0].item())
        W = int(entry["width"][0].item())
        fps_t = entry.get("fps", None)
        if fps_t is not None and not torch.all(fps_t == fps_t[0]):
            logger.warning(f"Different FPS in batch: {fps_t.tolist()}, using first.")
        fps = float((fps_t[0].item() if fps_t is not None else DEFAULT_FPS))
        return lat, F, H, W, fps


def _unpack_condition_entry(conds: Any) -> tuple[Tensor, Tensor]:
    """
    conds ì˜ˆì‹œ
    1) scene-levelë§Œ ìˆëŠ” ê²½ìš°: conds = {"prompt_embeds": (B, T, D), "prompt_attention_mask": (B, T)}
    2) prev/curr êµ¬ë¶„ ì €ì¥: conds = {
            "current_shot": {"prompt_embeds":..., "prompt_attention_mask":...},
            "prev_shot":    {"prompt_embeds":..., "prompt_attention_mask":...} (ì˜µì…˜)
       }
    """

    return conds["prompt_embeds"], conds["prompt_attention_mask"]


def _concat_prev_curr(prev_lat: Tensor | None, curr_lat: Tensor) -> tuple[Tensor, int, int]:
    """
    prev_lat: (B, Pseq, D) ë˜ëŠ” None
    curr_lat: (B, Cseq, D)
    return: (concat, Pseq, Cseq)

    # concat = [prev(clean) | curr(noisy)] @ seq-dim
    """
    if prev_lat is None:
        return curr_lat, 0, curr_lat.shape[1]
    return torch.cat([prev_lat, curr_lat], dim=1), prev_lat.shape[1], curr_lat.shape[1]


# ----------------------------------------
# Standard: prev-shot ì§€ì› (ìƒˆ ê·œì•½ì— ë§ê²Œ)
# ----------------------------------------
class StandardTrainingStrategy(TrainingStrategy):
    def __init__(self, conditioning_config: ConditioningConfig):
        super().__init__(conditioning_config)

    def get_data_sources(self) -> list[str]:
        """
        í‘œì¤€ í•™ìŠµì— í•„ìš”í•œ ì†ŒìŠ¤:
        - "latents": {"current_shot": {...}, "prev_shot": {... or SOS}}  # â† Datasetì´ ë³´ì¥
        - "conditions": scene/text ì¡°ê±´ (prev êµ¬ë¶„ì´ ìˆë”ë¼ë„ currë§Œ ì“°ë©´ ì¶©ë¶„)
        """
        return {"latents": "latent_conditions", "conditions": "text_conditions"}
    def prepare_batch(self, batch: dict[str, Any], timestep_sampler: TimestepSampler) -> TrainingBatch:
        # 1) Latents ì–¸íŒ©
        curr_lat, F, H, W, fps = _unpack_latent_entry(batch["latent_conditions"])

        prev_lat = None
        if batch.get("prev_conditions", None) is not None:
            prev_lat, _, _, _, _ = _unpack_latent_entry(batch["prev_conditions"])

        # 2) Conditions ì–¸íŒ© (scene-levelì´ë©´ currì™€ ë™ì¼)
        prompt_embeds, prompt_attention_mask = _unpack_condition_entry(batch["text_conditions"])
        
        
        # print(f"-------------batch config-------------"
        #       f"curr_lat : {curr_lat.shape}"
        #       f"prev_lat : {prev_lat.shape}"
        #       f"prompt_embeds : {prompt_embeds.shape}"
        #       )

        # 3) ë…¸ì´ì¦ˆ ìƒ˜í”Œ & ì‹œê·¸ë§ˆ (currì—ë§Œ ì ìš©)
        sigmas = timestep_sampler.sample_for(curr_lat)         # (B, Cseq, 1) ë˜ëŠ” ì „ëµ êµ¬í˜„ì— ë”°ë¼
        # â†“ ì•„ë˜ ì—°ì‚°ì—ì„œ (B,1,1) ë¸Œë¡œë“œìºìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ reshape
        if sigmas.dim() > 3:
            raise ValueError("Unexpected sigma shape; expected (B,1,1) style broadcast.")
        sigmas = sigmas.view(curr_lat.shape[0], 1, 1)          # (B,1,1)

        noise = torch.randn_like(curr_lat, device=curr_lat.device)  # (B, Cseq, D)
        noisy_curr = (1 - sigmas) * curr_lat + sigmas * noise       # (B, Cseq, D)

        # 4) curr ë‚´ë¶€ 'ì²« í”„ë ˆì„ conditioning' ì ìš©: ì²« í”„ë ˆì„ í† í°ì€ í´ë¦°ìœ¼ë¡œ ëŒ€ì²´
        first_mask_curr = self._create_first_frame_conditioning_mask(
            batch_size=curr_lat.shape[0],
            sequence_length=curr_lat.shape[1],
            height=H,
            width=W,
            device=curr_lat.device,
        )  # (B, Cseq) True=clean keep
        noisy_curr = torch.where(first_mask_curr.unsqueeze(-1), curr_lat, noisy_curr)

        # 5) prev + curr concat
        concat_lat, Pseq, Cseq = _concat_prev_curr(prev_lat, noisy_curr)  # (B, P+C, D)

        # 6) conditioning mask êµ¬ì„±
        #    prev ì „ì²´ True(ì¡°ê±´), currì€ first_frameë§Œ True
        if Pseq > 0:
            prev_mask = torch.ones(curr_lat.shape[0], Pseq, dtype=torch.bool, device=curr_lat.device)  # (B, Pseq)=True
            conditioning_mask = torch.cat([prev_mask, first_mask_curr], dim=1)  # (B, P+C)
        else:
            conditioning_mask = first_mask_curr  # (B, C)

        # 7) íƒ€ê¹ƒ êµ¬ì„±: prev êµ¬ê°„ì€ 0 (ë§ˆìŠ¤í‚¹ë˜ë¯€ë¡œ ì˜í–¥ X), curr êµ¬ê°„ì€ (noise - clean)
        targets_curr = noise - curr_lat  # (B, Cseq, D)
        if Pseq > 0:
            zeros_prev = torch.zeros(curr_lat.shape[0], Pseq, curr_lat.shape[2], device=curr_lat.device, dtype=targets_curr.dtype)
            targets = torch.cat([zeros_prev, targets_curr], dim=1)  # (B, P+C, D)
        else:
            targets = targets_curr  # (B, C, D)

        # 8) timestep ìƒì„±: prev=0, curr=round(sigmas*1000)
        sampled_t = torch.round(sigmas.squeeze(-1).squeeze(-1) * 1000.0).long()  # (B,)
        timesteps = self._create_timesteps_from_conditioning_mask(conditioning_mask, sampled_t)  # (B, P+C)

        # 9) ROPE scale & video coords (prev+curr ê¸¸ì´ì— ë§ì¶”ì–´ ì¤€ë¹„)
        rope_scale = get_rope_scale_factors(fps)
        # seq_mult = 2 if Pseq > 0 else 1  # prevë¥¼ ë¶™ì´ë©´ 2ë°°
        
        
        if Pseq > 0:
            raw_coords = prepare_video_coordinates(
                num_frames=F, height=H, width=W,
                batch_size=concat_lat.shape[0],
                sequence_multiplier=2,              # â˜… í•µì‹¬: prev + curr
                device=concat_lat.device,
            )
            prescaled_f = raw_coords[..., 0] * rope_scale[0]
            prescaled_h = raw_coords[..., 1] * rope_scale[1]
            prescaled_w = raw_coords[..., 2] * rope_scale[2]
            video_coords = torch.stack([prescaled_f, prescaled_h, prescaled_w], dim=1)  # (B, 3, P+C)
        else:
            video_coords = None

        # return TrainingBatch(
        #     latents=concat_lat,
        #     targets=targets,
        #     prompt_embeds=prompt_embeds,
        #     prompt_attention_mask=prompt_attention_mask,
        #     timesteps=timesteps,
        #     sigmas=sigmas,
        #     conditioning_mask=conditioning_mask,
        #     num_frames=F,
        #     height=H,
        #     width=W,
        #     fps=fps,
        #     rope_interpolation_scale=rope_scale,
        #     video_coords=video_coords,
        # )
        
        return TrainingBatch(
            latents=concat_lat,
            targets=targets,
            prompt_embeds=prompt_embeds,
            prompt_attention_mask=prompt_attention_mask,
            timesteps=timesteps,
            sigmas=sigmas,
            conditioning_mask=conditioning_mask,
            num_frames=F,
            height=H,
            width=W,
            fps=fps,
            rope_interpolation_scale=rope_scale,
            video_coords=video_coords,
        )
        
        
        

    def compute_loss(self, model_pred: Tensor, batch: TrainingBatch) -> Tensor:
        """
        ë§ˆìŠ¤í‚¹ MSE
        - prev ì „ì²´ & currì˜ ì²« í”„ë ˆì„(ì¡°ê±´)ì€ ì œì™¸
        - targetsëŠ” prev êµ¬ê°„ 0ìœ¼ë¡œ ì±„ì›Œì ¸ ìˆìŒ (ì•ˆì „)
        """
        loss = (model_pred - batch.targets).pow(2)                    # (B, Seq, D)
        loss_mask = (~batch.conditioning_mask.unsqueeze(-1)).float()  # (B, Seq, 1)
        loss = loss.mul(loss_mask).div(loss_mask.mean())              # í‰ê·  ì •ê·œí™”
        return loss.mean()


# ------------------------------------------------------
# ReferenceVideo (IC-LoRA): prev-shot ìë™ í´ë°± ì§€ì›


# --------------- Factory ---------------
def get_training_strategy(conditioning_config: ConditioningConfig) -> TrainingStrategy:
    mode = conditioning_config.mode
    if mode == "none":
        strategy = StandardTrainingStrategy(conditioning_config)
    elif mode == "reference_video":
        strategy = ReferenceVideoTrainingStrategy(conditioning_config)
    else:
        raise ValueError(f"Unknown conditioning mode: {mode}")

    logger.debug(f"ğŸ¯ Using {strategy.__class__.__name__}")
    return strategy



if __name__ == "__main__":
    import os
    import time
    import torch
    from torch.utils.data import DataLoader

    # --------------------------
    # 0) ì¬í˜„/ë””ë°”ì´ìŠ¤ ì„¤ì •
    # --------------------------
    torch.manual_seed(42)
    torch.backends.cudnn.benchmark = False
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --------------------------
    # 1) ë°ì´í„°ì…‹ ë¡œë“œ
    # --------------------------
    from ltxv_trainer.SG_datasets import PrecomputedDataset


    data_root = "/home/jeongseon38/datasets/videos/splits/.precomputed"
    
    print("loading data!")
    # DataLoaderëŠ” ì—¬ê¸°ì„  1ê°œë§Œ ë½‘ì•„ë³´ëŠ” ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ìš©ë„ë¡œ ì‚¬ìš©
    ds = PrecomputedDataset(data_root)
    dl = DataLoader(ds, batch_size=1, shuffle=True, num_workers=0, pin_memory=False)
    print("data successfully loaded!")

    # --------------------------
    # 2) ì„¤ì •(ì „ëµ/ìƒ˜í”ŒëŸ¬)
    # --------------------------


    # ëª¨ë“œ ì„ íƒ: "none" â†’ Standard, "reference_video" â†’ IC-LoRA ìŠ¤íƒ€ì¼
    cfg = ConditioningConfig(
        mode="none",                      # "none" | "reference_video"
        first_frame_conditioning_p=0.5,   # ì²« í”„ë ˆì„ì„ conditioningìœ¼ë¡œ ì‚¬ìš©í•  í™•ë¥ 
        # reference_latents_dir="ref_latents",  # í•„ìš” ì‹œ í™œì„±í™”
    )
    sampler = UniformTimestepSampler(min_value=0.0, max_value=1.0)


    # ì „ëµ ìƒì„±
    strategy = get_training_strategy(cfg)
    print(f"[INFO] Using strategy: {strategy.__class__.__name__}")

    # --------------------------
    # 3) ë°°ì¹˜ â†’ TrainingBatch
    # --------------------------
    t0 = time.time()
    raw_batch = next(iter(dl))  # Datasetì´ dictë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
    t1 = time.time()

    # ì£¼ì˜: Datasetì—ì„œ ë½‘íŒ í…ì„œë“¤ì´ CPUì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„ 
    # prepare_batch ë‚´ë¶€ì—ì„œì˜ ë””ë°”ì´ìŠ¤ ê°€ì •ì— ë§ì¶° ê·¸ëŒ€ë¡œ ì „ë‹¬.
    # (ì „ëµ ë‚´ë¶€ ì—°ì‚°ì€ ì…ë ¥ í…ì„œì˜ deviceë¥¼ ì‚¬ìš©)

    training_batch = strategy.prepare_batch(raw_batch, sampler)
    t2 = time.time()

    print("------------- Prepared TrainingBatch -------------")
    print(f"latents              : {tuple(training_batch.latents.shape)}  (B, Seq, D)")
    print(f"targets              : {tuple(training_batch.targets.shape)}")
    print(f"prompt_embeds        : {tuple(training_batch.prompt_embeds.shape)}")
    print(f"prompt_attention_mask: {tuple(training_batch.prompt_attention_mask.shape)}")
    print(f"timesteps            : {tuple(training_batch.timesteps.shape)}")
    print(f"sigmas               : {tuple(training_batch.sigmas.shape)}")
    print(f"conditioning_mask    : {tuple(training_batch.conditioning_mask.shape)}  (True=conditioning)")
    print(f"num_frames / HxW / fps: {training_batch.num_frames} / {training_batch.height}x{training_batch.width} / {training_batch.fps}")
    if training_batch.video_coords is not None:
        print(f"video_coords         : {tuple(training_batch.video_coords.shape)}  (B, 3, Seq)")
    print(f"rope_scale           : {training_batch.rope_interpolation_scale}")
    print("--------------------------------------------------")
    print(f"[Timing] dataloader: {(t1 - t0):.3f}s, prepare_batch: {(t2 - t1):.3f}s")

    # --------------------------
    # 4) ë”ë¯¸ ëª¨ë¸ë¡œ forward & loss
    # --------------------------
    # ì‹¤ì œ ëª¨ë¸ ì…ë ¥ ê·œê²©ì— ë§ì¶° dictë¥¼ êµ¬ì„±
    model_inputs = strategy.prepare_model_inputs(training_batch)
    # ì‹¤ì œ ëª¨ë¸ì´ ì—†ë‹¤ë©´, ë™ì¼ shapeì˜ ë”ë¯¸ ì˜ˆì¸¡ì„ ìƒì„±(í‰ê·  0, ë¶„ì‚° ë™ì¼)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ targetsì— ë…¸ì´ì¦ˆë¥¼ ë”í•œ ê°’ì„ ì˜ˆì¸¡ìœ¼ë¡œ ì‚¬ìš©
    with torch.no_grad():
        model_pred = training_batch.targets + 0.05 * torch.randn_like(training_batch.targets)

    loss = strategy.compute_loss(model_pred, training_batch)
    print(f"[Loss] masked MSE: {loss.item():.6f}")

    # --------------------------
    # 5) (ì„ íƒ) GPU ì´ë™ í…ŒìŠ¤íŠ¸
    # --------------------------
    # ì‹¤ì œ í•™ìŠµ ì½”ë“œì—ì„  ëª¨ë¸/ë°°ì¹˜ í…ì„œë¥¼ accelerator.deviceë¡œ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„  ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì°¨ì›ì—ì„œ latentsë§Œ ì ê¹ ì˜®ê²¨ë³´ê¸°:
    try:
        if torch.cuda.is_available():
            _ = training_batch.latents.to(device)
            print(f"[Device] Latents moved to {device} OK.")
    except Exception as e:
        print(f"[WARN] Device move test failed: {e}")

    print("[DONE] Strategy smoke test finished.")



    